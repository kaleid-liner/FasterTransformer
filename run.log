[FT][INFO] T5EncoderOp.cc: topk = 1
[FT][WARNING] Skip NCCL initialization since requested tensor/pipeline parallel sizes are equals to 1.
[WARNING] gemm_config.in is not found; using default GEMM algo
[FT][WARNING] Skip NCCL initialization since requested tensor/pipeline parallel sizes are equals to 1.
[WARNING] gemm_config.in is not found; using default GEMM algo
moe_layers_in_encoder:  [1, 3, 5, 7, 9, 11]

=============== Argument ===============
batch_size: 1
beam_width: 4
seq_len: 256
input_seq_len: 0
test_time: 3
beam_search_diversity_rate: 0.0
sampling_topk: 1
sampling_topp: 0.0
data_type: fp32
load_data_type: fp32
lib_path: lib/libth_transformer.so
model_path: None
model: t5-base
tensor_para_size: 1
pipeline_para_size: 1
ckpt_path: /workspace/FasterTransformer/fake_t5_moe_ckpt/
model_type: Megatron-DeepSpeed
warmup_iterations: 1
iterations: 4
duration: 0
seed: 0
skip_gemm: False
========================================
Megatron-DeepSpeed encoder_config: T5Config {
  "_name_or_path": "t5-base",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": false,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "moe_layer_index": [
    1,
    3,
    5,
    7,
    9,
    11
  ],
  "n_positions": 512,
  "num_decoder_layers": 12,
  "num_experts": 8,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "transformers_version": "4.27.1",
  "use_cache": false,
  "vocab_size": 32128
}

Megatron-DeepSpeed decoder_config: T5Config {
  "_name_or_path": "t5-base",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_decoder": true,
  "is_encoder_decoder": false,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "moe_layer_index": [
    1,
    3,
    5,
    7,
    9,
    11
  ],
  "n_positions": 512,
  "num_decoder_layers": 12,
  "num_experts": 8,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "transformers_version": "4.27.1",
  "use_cache": true,
  "vocab_size": 32128
}

Run gemm test: ./bin/t5_gemm 1 1 128 768 12 64 3072 768 12 64 3072 32128 0 1 1 1 > .tmp_gemm.log
[INFO] WARNING: Exception occurred in dist.init_process_group(backend = 'mpi'). Maybe the process group has been initialized somewhere else.
[INFO] WARNING: Exception occurred in dist.init_process_group(backend = 'mpi'). Maybe the process group has been initialized somewhere else.
[INFO] Load weights for encoder from CPP.
[INFO] Load weights for decoding from CPP.
perf_benchmark topk =  1
[INFO] WARNING: Exception occurred in dist.init_process_group(backend = 'mpi'). Maybe the process group has been initialized somewhere else.
[INFO] WARNING: Exception occurred in dist.init_process_group(backend = 'mpi'). Maybe the process group has been initialized somewhere else.
perf_benchmark.py: task0
perf_benchmark.py: 1/1
===== ft encoder forward start
arena_size: 20971520
encoder_fetcher_mode: 1
decoder_fetcher_mode: 2
profiling: 1
offload_path: /data/ft/switch-base-8/
disk_offload: 0
load_from_cpp: 1
use_cache: 1
quant_mode: 0
vocab_size: 32128
[FT][INFO] fetcher_sync_wait_time 0 us
[FT][INFO] calc_sparse_time 31 us
[FT][INFO] expert_for_row_backup_time 0 us
[FT][INFO] expert_for_row_time 84 us
[FT][INFO] total_row_cpy 1536
[FT][INFO] layer_1_fetch_time 0 us
Total events num: (comp)6 (mem)6
Comp avg lats: 1.15985 ms
Mem avg lats: 26.6033 ms
Average cache hit rate: 0.113636
===== ft encoder forward finished
===== ft decoder forward start
arena_size: 20971520
encoder_fetcher_mode: 1
decoder_fetcher_mode: 2
profiling: 1
offload_path: /data/ft/switch-base-8/
disk_offload: 0
load_from_cpp: 1
use_cache: 1
quant_mode: 0
vocab_size: 32128
[FT][INFO] fetcher_sync_wait_time 0 us
[FT][INFO] calc_sparse_time 33 us
[FT][INFO] expert_for_row_backup_time 20498 us
[FT][INFO] expert_for_row_time 17758 us
[FT][INFO] total_row_cpy 3072
[FT][INFO] layer_1_fetch_time 1009644 us
Total events num: (comp)1536 (mem)1536
Comp avg lats: 0.0222339 ms
Mem avg lats: 3.93361 ms
Average cache hit rate: 0
===== ft decoder forward finished
perf_benchmark.py: task1
perf_benchmark.py: 1/4
===== ft encoder forward start
arena_size: 20971520
encoder_fetcher_mode: 1
decoder_fetcher_mode: 2
profiling: 1
offload_path: /data/ft/switch-base-8/
disk_offload: 0
load_from_cpp: 1
use_cache: 1
quant_mode: 0
vocab_size: 32128
[FT][INFO] fetcher_sync_wait_time 0 us
[FT][INFO] calc_sparse_time 64 us
[FT][INFO] expert_for_row_backup_time 20498 us
[FT][INFO] expert_for_row_time 17845 us
[FT][INFO] total_row_cpy 4608
[FT][INFO] layer_1_fetch_time 1009644 us
Total events num: (comp)6 (mem)6
Comp avg lats: 0.91648 ms
Mem avg lats: 24.3915 ms
Average cache hit rate: 0.0930233
===== ft encoder forward finished
===== ft decoder forward start
arena_size: 20971520
encoder_fetcher_mode: 1
decoder_fetcher_mode: 2
profiling: 1
offload_path: /data/ft/switch-base-8/
disk_offload: 0
load_from_cpp: 1
use_cache: 1
quant_mode: 0
vocab_size: 32128
[FT][INFO] fetcher_sync_wait_time 0 us
[FT][INFO] calc_sparse_time 69 us
[FT][INFO] expert_for_row_backup_time 41552 us
[FT][INFO] expert_for_row_time 35820 us
[FT][INFO] total_row_cpy 6144
[FT][INFO] layer_1_fetch_time 2039533 us
Total events num: (comp)1536 (mem)1536
Comp avg lats: 0.0207879 ms
Mem avg lats: 3.95916 ms
Average cache hit rate: 0
===== ft decoder forward finished
perf_benchmark.py: 2/4
===== ft encoder forward start
arena_size: 20971520
encoder_fetcher_mode: 1
decoder_fetcher_mode: 2
profiling: 1
offload_path: /data/ft/switch-base-8/
disk_offload: 0
load_from_cpp: 1
use_cache: 1
quant_mode: 0
vocab_size: 32128
[FT][INFO] fetcher_sync_wait_time 0 us
[FT][INFO] calc_sparse_time 97 us
[FT][INFO] expert_for_row_backup_time 41552 us
[FT][INFO] expert_for_row_time 35907 us
[FT][INFO] total_row_cpy 7680
[FT][INFO] layer_1_fetch_time 2039533 us
Total events num: (comp)6 (mem)6
Comp avg lats: 0.913067 ms
Mem avg lats: 25.3423 ms
Average cache hit rate: 0.0909091
===== ft encoder forward finished
===== ft decoder forward start
arena_size: 20971520
encoder_fetcher_mode: 1
decoder_fetcher_mode: 2
profiling: 1
offload_path: /data/ft/switch-base-8/
disk_offload: 0
load_from_cpp: 1
use_cache: 1
quant_mode: 0
vocab_size: 32128
[FT][INFO] fetcher_sync_wait_time 0 us
[FT][INFO] calc_sparse_time 97 us
[FT][INFO] expert_for_row_backup_time 62311 us
[FT][INFO] expert_for_row_time 53705 us
[FT][INFO] total_row_cpy 9216
[FT][INFO] layer_1_fetch_time 3089432 us
Total events num: (comp)1536 (mem)1536
Comp avg lats: 0.019899 ms
Mem avg lats: 3.96274 ms
Average cache hit rate: 0
===== ft decoder forward finished
perf_benchmark.py: 3/4
===== ft encoder forward start
arena_size: 20971520
encoder_fetcher_mode: 1
decoder_fetcher_mode: 2
profiling: 1
offload_path: /data/ft/switch-base-8/
disk_offload: 0
load_from_cpp: 1
use_cache: 1
quant_mode: 0
vocab_size: 32128
[FT][INFO] fetcher_sync_wait_time 0 us
[FT][INFO] calc_sparse_time 124 us
[FT][INFO] expert_for_row_backup_time 62311 us
[FT][INFO] expert_for_row_time 53788 us
[FT][INFO] total_row_cpy 10752
[FT][INFO] layer_1_fetch_time 3089432 us
Total events num: (comp)6 (mem)6
Comp avg lats: 0.912043 ms
Mem avg lats: 21.7236 ms
Average cache hit rate: 0.162791
===== ft encoder forward finished
===== ft decoder forward start
arena_size: 20971520
encoder_fetcher_mode: 1
decoder_fetcher_mode: 2
profiling: 1
offload_path: /data/ft/switch-base-8/
disk_offload: 0
load_from_cpp: 1
use_cache: 1
quant_mode: 0
vocab_size: 32128
[FT][INFO] fetcher_sync_wait_time 0 us
[FT][INFO] calc_sparse_time 124 us
[FT][INFO] expert_for_row_backup_time 83220 us
[FT][INFO] expert_for_row_time 71708 us
[FT][INFO] total_row_cpy 12288
[FT][INFO] layer_1_fetch_time 4160349 us
Total events num: (comp)1536 (mem)1536
Comp avg lats: 0.0203932 ms
Mem avg lats: 3.97442 ms
Average cache hit rate: 0
===== ft decoder forward finished
perf_benchmark.py: 4/4
===== ft encoder forward start
arena_size: 20971520
encoder_fetcher_mode: 1
decoder_fetcher_mode: 2
profiling: 1
offload_path: /data/ft/switch-base-8/
disk_offload: 0
load_from_cpp: 1
use_cache: 1
quant_mode: 0
vocab_size: 32128
[FT][INFO] fetcher_sync_wait_time 0 us
[FT][INFO] calc_sparse_time 153 us
[FT][INFO] expert_for_row_backup_time 83220 us
[FT][INFO] expert_for_row_time 71790 us
[FT][INFO] total_row_cpy 13824
[FT][INFO] layer_1_fetch_time 4160349 us
Total events num: (comp)6 (mem)6
Comp avg lats: 0.936789 ms
Mem avg lats: 27.8456 ms
Average cache hit rate: 0.108696
===== ft encoder forward finished
===== ft decoder forward start
arena_size: 20971520
encoder_fetcher_mode: 1
decoder_fetcher_mode: 2
profiling: 1
offload_path: /data/ft/switch-base-8/
disk_offload: 0
load_from_cpp: 1
use_cache: 1
quant_mode: 0
vocab_size: 32128
[FT][INFO] fetcher_sync_wait_time 0 us
[FT][INFO] calc_sparse_time 153 us
[FT][INFO] expert_for_row_backup_time 104252 us
[FT][INFO] expert_for_row_time 89685 us
[FT][INFO] total_row_cpy 15360
[FT][INFO] layer_1_fetch_time 5216519 us
Total events num: (comp)1536 (mem)1536
Comp avg lats: 0.0209339 ms
Mem avg lats: 3.92615 ms
Average cache hit rate: 0
===== ft decoder forward finished
[INFO] ft-sampling translates 4 batches taking 25.81 sec to translate 1024 tokens (6453.5300 ms per batch), 40 tokens/sec.
